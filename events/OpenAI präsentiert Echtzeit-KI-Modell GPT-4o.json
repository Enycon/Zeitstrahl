{
    "group": "AI",
    "content": "OpenAI präsentiert Echtzeit-KI-Modell GPT-4o",
    "short_title": "GPT-4o",
    "start": "2024-05-13",
    "details": "### GPT-4o: Ein Schritt in Richtung natürlicher Mensch-Maschine-Interaktion\n\nIm Mai 2024 stellte OpenAI GPT-4o (\"o\" für \"omni\") vor, sein erstes Modell, das von Grund auf multimodal konzipiert wurde. Es verarbeitet Text, Audio und Bilder als ein einziges, nahtlos integriertes System, was zu einer Interaktion in Echtzeit führt.\n\n#### Die Rolle der KI im Durchbruch\n\nDer Durchbruch von GPT-4o liegt in seiner einheitlichen KI-Architektur, die die bisherigen Latenzzeiten bei der Sprachinteraktion eliminiert. Während frühere Modelle Audio in Text umwandeln, diesen verarbeiten und die Antwort wieder in Audio umwandeln mussten, verarbeitet GPT-4o den Audiostrom direkt. Dies ermöglicht flüssige Gespräche, bei denen die KI Emotionen im Tonfall erkennen, unterbrochen werden und sogar lachen oder singen kann. Die Demonstrationen zeigten eine beispiellos natürliche und menschenähnliche Interaktionsfähigkeit.\n\n#### Bedeutung und potenzieller Einfluss\n\nGPT-4o definiert die Zukunft von KI-Assistenten neu, indem es den Fokus von textbasierten Chats auf intuitive, sprachgesteuerte Interaktionen verlagert. OpenAI machte die Intelligenz auf GPT-4-Niveau mit diesem Modell zudem für alle kostenlosen ChatGPT-Nutzer verfügbar und demokratisierte damit den Zugang zu Spitzen-KI. Dies erhöht den Druck auf Konkurrenten wie Apple und Google, ihre eigenen digitalen Assistenten grundlegend zu überarbeiten."
}