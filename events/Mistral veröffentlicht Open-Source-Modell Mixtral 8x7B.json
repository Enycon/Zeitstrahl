{
    "group": "AI",
    "content": "Mistral veröffentlicht Open-Source-Modell Mixtral 8x7B",
    "short_title": "Mixtral 8x7B",
    "start": "2023-12-11",
    "details": "### Mixtral 8x7B: Die Macht der Experten für die Open-Source-Welt\n\nIm Dezember 2023 veröffentlichte Mistral AI mit Mixtral 8x7B ein weiteres bahnbrechendes Open-Source-Modell. Es war eines der ersten hochleistungsfähigen Modelle, das eine **Mixture-of-Experts (MoE)**-Architektur nutzte. Obwohl es insgesamt 47 Milliarden Parameter hatte, aktivierte es pro Inferenzschritt nur etwa 13 Milliarden, was ihm die Geschwindigkeit und die Kosten eines viel kleineren Modells verlieh.\n\n#### Die Rolle der KI im Durchbruch\n\nDie MoE-Architektur ist der Kern des Durchbruchs. Anstatt eines riesigen, monolithischen neuronalen Netzes besteht Mixtral aus acht kleineren \"Experten\"-Netzwerken (jeweils 7B Parameter). Ein Routing-Netzwerk entscheidet bei jeder Anfrage, welche zwei Experten am besten geeignet sind, um die Aufgabe zu lösen. Diese Spezialisierung ermöglicht eine wesentlich effizientere Nutzung der Rechenressourcen.\n\n#### Bedeutung und potenzieller Einfluss\n\nMixtral 8x7B war ein Meilenstein, da es Open-Source-Nutzern erstmals die Leistung eines Modells auf dem Niveau von GPT-3.5 bot, und das unter der freizügigen Apache-2.0-Lizenz. Es setzte einen neuen Standard für das Verhältnis von Kosten zu Leistung und bewies, dass die fortschrittliche MoE-Architektur nicht nur den großen proprietären Laboren vorbehalten ist. Es festigte Mistrals Ruf als Innovationsführer im Open-Source-KI-Bereich."
}